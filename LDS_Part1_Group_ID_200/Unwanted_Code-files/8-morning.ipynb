{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import csv\n",
    "import pyodbc\n",
    "from geopy.geocoders import Nominatim\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load participant age, status, and type dictionaries\n",
    "with open('dict_partecipant_age.json') as f1:\n",
    "    dict_partecipant_age = json.load(f1)\n",
    "\n",
    "with open('dict_partecipant_status.json') as f2:\n",
    "    dict_partecipant_status = json.load(f2)\n",
    "\n",
    "with open('dict_partecipant_type.json') as f3:\n",
    "    dict_partecipant_type = json.load(f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute additional date-related data\n",
    "def compute_date_data(date_str):\n",
    "    date_obj = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n",
    "    date = date_obj.date()\n",
    "    day = date_obj.day\n",
    "    month = date_obj.month\n",
    "    year = date_obj.year\n",
    "    quarter = (date_obj.month - 1) // 3 + 1\n",
    "    day_of_week = date_obj.strftime('%A')\n",
    "    return date, day, month, year, quarter, day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse dates.xml and create a mapping of date_fk to real date\n",
    "def parse_dates_xml(xml_file):\n",
    "    date_mapping = {}\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for row in root.findall('.//row'):\n",
    "        date = row.find('date').text\n",
    "        date_pk = int(row.find('date_pk').text)\n",
    "        date_mapping[date_pk] = date\n",
    "\n",
    "    return date_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute crime gravity using provided dictionaries\n",
    "def compute_crime_gravity(x):\n",
    "    gravity = dict_partecipant_age.get(x['participant_age_group'], 1) * \\\n",
    "              dict_partecipant_type.get(x['participant_type'], 1) * \\\n",
    "              dict_partecipant_status.get(x['participant_status'], 1)\n",
    "    return gravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continent_by_country_code(country_code):\n",
    "    if country_code:\n",
    "        try:\n",
    "            response = requests.get(f'https://restcountries.com/v3/alpha/{country_code.lower()}')\n",
    "            data = response.json()\n",
    "            continent = data[0]['region']\n",
    "            return continent\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching continent information: {e}\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location_info(latitude, longitude):\n",
    "    geolocator = Nominatim(user_agent=\"Lab_DSS_Group_ID_200\")\n",
    "    location = geolocator.reverse((latitude, longitude), language='en')\n",
    "\n",
    "    if location is not None:\n",
    "        address = location.address\n",
    "        city = location.raw.get('address', {}).get('city') or location.raw.get('address', {}).get('town') or location.raw.get('address', {}).get('village') or location.raw.get('address', {}).get('county', None)\n",
    "\n",
    "        state = location.raw.get('address', {}).get('state', None)\n",
    "        #country = location.raw.get('address', {}).get('country', None) (Not calculated as the country is USA for all records, and the Continent is North America)\n",
    "\n",
    "        #continent = get_continent_by_country_code(location.raw.get('address', {}).get('country_code', None)) \n",
    "\n",
    "        return {\n",
    "            'city': city,\n",
    "            'state': state,\n",
    "            #'country': country,\n",
    "            #'continent': continent\n",
    "        }\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to my db\n",
    "# Connection string\n",
    "server = 'tcp:lds.di.unipi.it'\n",
    "username = 'Group_ID_200'\n",
    "password = '89VIG10K'\n",
    "database = 'Group_ID_200_DB'\n",
    "connectionString = 'DRIVER={ODBC Driver 17 for SQL Server};SERVER=' + server + ';DATABASE=' + database + ';UID=' + username + ';PWD=' + password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQL Server database\n",
    "conn = pyodbc.connect(connectionString)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_id_dict = {}\n",
    "gun_id_dict = {}\n",
    "partecipant_id_dict = {}\n",
    "date_id_dict = {}\n",
    "incident_id_dict = {}\n",
    "custody_id_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates.xml to get date mapping\n",
    "date_mapping = parse_dates_xml('dates.xml')\n",
    "\n",
    "def bulk_insert_data(conn, cursor, table_name, columns, values_list):\n",
    "    placeholders = ', '.join(['?' for _ in range(len(columns))])\n",
    "    insert_query = f'INSERT INTO {table_name} ({columns}) VALUES ({placeholders});'\n",
    "    cursor.executemany(insert_query, values_list)\n",
    "    conn.commit()\n",
    "\n",
    "def get_surrogate_key(conn, cursor, table_name, key_dict):\n",
    "    columns = ', '.join(key_dict.keys())\n",
    "    placeholders = ', '.join(['?'] * len(key_dict))\n",
    "    select_query = f'SELECT * FROM {table_name} WHERE ({columns}) = ({placeholders});'\n",
    "\n",
    "    cursor.execute(select_query, list(key_dict.values()))\n",
    "    result = cursor.fetchone()\n",
    "\n",
    "    if result:\n",
    "        return result[0]  # Assuming the surrogate key is in the first column\n",
    "    else:\n",
    "        insert_query = f'INSERT INTO {table_name} ({columns}) VALUES ({placeholders});'\n",
    "        cursor.execute(insert_query, list(key_dict.values()))\n",
    "        conn.commit()\n",
    "\n",
    "        cursor.execute(select_query, list(key_dict.values()))\n",
    "        return cursor.fetchone()[0]\n",
    "\n",
    "def split_and_integrate(csv_file):\n",
    "    \n",
    "    table_names = ['Custody', 'Geography', 'Gun', 'Date', 'Incident', 'Partecipant']\n",
    "\n",
    "    for table_name in table_names:\n",
    "        cursor.execute(f'DELETE FROM {table_name}')\n",
    "        conn.commit()\n",
    "\n",
    "    batch_size = 50\n",
    "    batch_data = []\n",
    "\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        next(reader)\n",
    "\n",
    "        for row in reader:\n",
    "            gun_stolen_bool = 1 if row[\"gun_stolen\"] == 'Stolen' else 0\n",
    "            gun_key_dict = {\"is_stolen\": gun_stolen_bool, \"gun_type\": row['gun_type']}\n",
    "            gun_id = get_surrogate_key(conn, cursor, 'Gun', gun_key_dict)\n",
    "\n",
    "            partecipant_key = {\n",
    "                \"age_group\": row['participant_age_group'],\n",
    "                \"gender\": row['participant_gender'],\n",
    "                \"type\": row['participant_type'],\n",
    "                \"status\": row['participant_status']\n",
    "            }\n",
    "            partecipant_id = get_surrogate_key(conn, cursor, 'Partecipant', partecipant_key)\n",
    "\n",
    "            latitude, longitude = float(row['latitude']), float(row['longitude'])\n",
    "            location_info = get_location_info(latitude, longitude)\n",
    "            city = location_info[\"city\"]\n",
    "            state = location_info[\"state\"]\n",
    "            country = \"United States\"\n",
    "            continent = \"North America\"\n",
    "            geo_key = {\n",
    "                \"latitude\": str(latitude), \"longitude\": str(longitude), \"city\": city, \"state\": state, \"country\": country,\n",
    "                \"continent\": continent\n",
    "            }\n",
    "            geo_id = get_surrogate_key(conn, cursor, 'Geography', geo_key)\n",
    "\n",
    "            date_id = int(row['date_fk'])\n",
    "            date_value = date_mapping[date_id]\n",
    "            date, day, month, year, quarter, day_of_week = compute_date_data(date_value)\n",
    "            date_key = {\n",
    "                \"date_id\": date_id, \"the_date\": date, \"the_day\": day, \"the_month\": month, \"the_year\": year,\n",
    "                \"quarter\": quarter, \"day_of_week\": day_of_week\n",
    "            }\n",
    "            insert_data_with_ID(conn, cursor, date_id_dict, \"Date\", date_key)\n",
    "\n",
    "            incident_id = int(row['incident_id'])\n",
    "            incident_key = {\"incident_id\": incident_id}\n",
    "            insert_data_with_ID(conn, cursor, incident_id_dict, \"Incident\", incident_key)\n",
    "\n",
    "            custody_key = {\n",
    "                \"custody_id\": row['custody_id'], \"partecipant_id\": partecipant_id, \"gun_id\": gun_id, \"geo_id\": geo_id,\n",
    "                \"date_id\": date_id, \"crime_gravity\": compute_crime_gravity(row), \"incident_id \": incident_id\n",
    "            }\n",
    "\n",
    "            batch_data.append(tuple(custody_key.values()))\n",
    "\n",
    "            row_count += 1\n",
    "\n",
    "            if row_count % batch_size == 0:\n",
    "                bulk_insert_data(conn, cursor, 'Custody', columns, batch_data)\n",
    "                batch_data = []\n",
    "\n",
    "    bulk_insert_data(conn, cursor, 'Custody', columns, batch_data)\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "split_and_integrate('Police.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IntegrityError",
     "evalue": "('23000', \"[23000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Cannot insert the value NULL into column 'partecipant_id', table 'Group_ID_200_DB.Group_ID_200.Custody'; column does not allow nulls. INSERT fails. (515) (SQLExecDirectW); [23000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]The statement has been terminated. (3621)\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIntegrityError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\LDS\\OneDrive - University of Pisa\\Sem-3\\DSS_LAB\\My Project Files\\LDS_Project_2023\\Main Project Files\\1st Assignment\\8-morning.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/LDS/OneDrive%20-%20University%20of%20Pisa/Sem-3/DSS_LAB/My%20Project%20Files/LDS_Project_2023/Main%20Project%20Files/1st%20Assignment/8-morning.ipynb#X13sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m conn\u001b[39m.\u001b[39mcommit()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/LDS/OneDrive%20-%20University%20of%20Pisa/Sem-3/DSS_LAB/My%20Project%20Files/LDS_Project_2023/Main%20Project%20Files/1st%20Assignment/8-morning.ipynb#X13sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39m# Call the function with the appropriate arguments\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/LDS/OneDrive%20-%20University%20of%20Pisa/Sem-3/DSS_LAB/My%20Project%20Files/LDS_Project_2023/Main%20Project%20Files/1st%20Assignment/8-morning.ipynb#X13sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m split_and_integrate(\u001b[39m'\u001b[39m\u001b[39mPolice.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/LDS/OneDrive%20-%20University%20of%20Pisa/Sem-3/DSS_LAB/My%20Project%20Files/LDS_Project_2023/Main%20Project%20Files/1st%20Assignment/8-morning.ipynb#X13sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39m# Close the database connection when done\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/LDS/OneDrive%20-%20University%20of%20Pisa/Sem-3/DSS_LAB/My%20Project%20Files/LDS_Project_2023/Main%20Project%20Files/1st%20Assignment/8-morning.ipynb#X13sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m cursor\u001b[39m.\u001b[39mclose()\n",
      "\u001b[1;32mc:\\Users\\LDS\\OneDrive - University of Pisa\\Sem-3\\DSS_LAB\\My Project Files\\LDS_Project_2023\\Main Project Files\\1st Assignment\\8-morning.ipynb Cell 11\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LDS/OneDrive%20-%20University%20of%20Pisa/Sem-3/DSS_LAB/My%20Project%20Files/LDS_Project_2023/Main%20Project%20Files/1st%20Assignment/8-morning.ipynb#X13sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m custody_id \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mcustody_id\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LDS/OneDrive%20-%20University%20of%20Pisa/Sem-3/DSS_LAB/My%20Project%20Files/LDS_Project_2023/Main%20Project%20Files/1st%20Assignment/8-morning.ipynb#X13sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m custody_key \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LDS/OneDrive%20-%20University%20of%20Pisa/Sem-3/DSS_LAB/My%20Project%20Files/LDS_Project_2023/Main%20Project%20Files/1st%20Assignment/8-morning.ipynb#X13sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcustody_id\u001b[39m\u001b[39m\"\u001b[39m: custody_id, \u001b[39m\"\u001b[39m\u001b[39mpartecipant_id\u001b[39m\u001b[39m\"\u001b[39m: participant_id, \u001b[39m\"\u001b[39m\u001b[39mgun_id\u001b[39m\u001b[39m\"\u001b[39m: gun_id, \u001b[39m\"\u001b[39m\u001b[39mgeo_id\u001b[39m\u001b[39m\"\u001b[39m: geo_id,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LDS/OneDrive%20-%20University%20of%20Pisa/Sem-3/DSS_LAB/My%20Project%20Files/LDS_Project_2023/Main%20Project%20Files/1st%20Assignment/8-morning.ipynb#X13sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdate_id\u001b[39m\u001b[39m\"\u001b[39m: date_id, \u001b[39m\"\u001b[39m\u001b[39mcrime_gravity\u001b[39m\u001b[39m\"\u001b[39m: compute_crime_gravity(row), \u001b[39m\"\u001b[39m\u001b[39mincident_id\u001b[39m\u001b[39m\"\u001b[39m: incident_id\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LDS/OneDrive%20-%20University%20of%20Pisa/Sem-3/DSS_LAB/My%20Project%20Files/LDS_Project_2023/Main%20Project%20Files/1st%20Assignment/8-morning.ipynb#X13sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/LDS/OneDrive%20-%20University%20of%20Pisa/Sem-3/DSS_LAB/My%20Project%20Files/LDS_Project_2023/Main%20Project%20Files/1st%20Assignment/8-morning.ipynb#X13sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m insert_data_with_ID(conn, cursor, custody_id_dict, \u001b[39m'\u001b[39m\u001b[39mCustody\u001b[39m\u001b[39m'\u001b[39m, custody_key)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LDS/OneDrive%20-%20University%20of%20Pisa/Sem-3/DSS_LAB/My%20Project%20Files/LDS_Project_2023/Main%20Project%20Files/1st%20Assignment/8-morning.ipynb#X13sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39m# Increment the row count\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LDS/OneDrive%20-%20University%20of%20Pisa/Sem-3/DSS_LAB/My%20Project%20Files/LDS_Project_2023/Main%20Project%20Files/1st%20Assignment/8-morning.ipynb#X13sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m row_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32mc:\\Users\\LDS\\OneDrive - University of Pisa\\Sem-3\\DSS_LAB\\My Project Files\\LDS_Project_2023\\Main Project Files\\1st Assignment\\8-morning.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LDS/OneDrive%20-%20University%20of%20Pisa/Sem-3/DSS_LAB/My%20Project%20Files/LDS_Project_2023/Main%20Project%20Files/1st%20Assignment/8-morning.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m placeholders \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39m'\u001b[39m\u001b[39m?\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(key_dict))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/LDS/OneDrive%20-%20University%20of%20Pisa/Sem-3/DSS_LAB/My%20Project%20Files/LDS_Project_2023/Main%20Project%20Files/1st%20Assignment/8-morning.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m insert_query \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mINSERT INTO \u001b[39m\u001b[39m{\u001b[39;00mtable_name\u001b[39m}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00mcolumns\u001b[39m}\u001b[39;00m\u001b[39m) VALUES (\u001b[39m\u001b[39m{\u001b[39;00mplaceholders\u001b[39m}\u001b[39;00m\u001b[39m);\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/LDS/OneDrive%20-%20University%20of%20Pisa/Sem-3/DSS_LAB/My%20Project%20Files/LDS_Project_2023/Main%20Project%20Files/1st%20Assignment/8-morning.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m cursor\u001b[39m.\u001b[39mexecute(insert_query, \u001b[39mlist\u001b[39m(key_dict\u001b[39m.\u001b[39mvalues()))\n",
      "\u001b[1;31mIntegrityError\u001b[0m: ('23000', \"[23000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Cannot insert the value NULL into column 'partecipant_id', table 'Group_ID_200_DB.Group_ID_200.Custody'; column does not allow nulls. INSERT fails. (515) (SQLExecDirectW); [23000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]The statement has been terminated. (3621)\")"
     ]
    }
   ],
   "source": [
    "# Function to insert data into the database with an automatically generated ID\n",
    "def insert_data_with_ID(conn, cursor, id_dict, table_name, key_dict):\n",
    "    key_tuple = tuple(key_dict.values())\n",
    "    if key_tuple not in id_dict:\n",
    "        id_dict[key_tuple] = next(iter(key_dict.values()))\n",
    "        columns = ', '.join(key_dict.keys())\n",
    "        placeholders = ', '.join(['?'] * len(key_dict))\n",
    "        insert_query = f'INSERT INTO {table_name} ({columns}) VALUES ({placeholders});'\n",
    "\n",
    "        cursor.execute(insert_query, list(key_dict.values()))\n",
    "\n",
    "# Function to get or insert ID into the dictionary\n",
    "def get_or_insert_id(conn, cursor, id_dict, table_name, key_dict):\n",
    "    key_tuple = tuple(key_dict.values())\n",
    "    if key_tuple in id_dict:\n",
    "        return id_dict[key_tuple]\n",
    "    else:\n",
    "        last_inserted_id = insert_data_with_ID(conn, cursor, id_dict, table_name, key_dict)\n",
    "        return last_inserted_id\n",
    "\n",
    "# Modified split_and_integrate function for batch inserts\n",
    "def split_and_integrate(csv_file):\n",
    "    # Parse dates.xml to get date mapping\n",
    "    date_mapping = parse_dates_xml('dates.xml')\n",
    "\n",
    "    # List of table names in your database\n",
    "    table_names = ['Custody', 'Geography', 'Gun', 'Date', 'Incident', 'Partecipant']\n",
    "\n",
    "    # Clean the tables by deleting all records\n",
    "    for table_name in table_names:\n",
    "        cursor.execute(f'DELETE FROM {table_name}')\n",
    "        conn.commit()\n",
    "\n",
    "    # Read and process Police.csv\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        next(reader)  # Skip the header row\n",
    "\n",
    "        # Set the batch size\n",
    "        batch_size = 10\n",
    "        batch_data = []\n",
    "\n",
    "        row_count = 0\n",
    "        # ... (previous code remains unchanged)\n",
    "\n",
    "        for row in reader:\n",
    "            custody_id, participant_age_group, participant_gender, participant_status, participant_type, latitude, longitude, gun_stolen, gun_type, incident_id, date_fk = row\n",
    "\n",
    "            gun_stolen_bool = 1 if row[\"gun_stolen\"] == 'Stolen' else 0\n",
    "            gun_key_dict = {\"is_stolen\": gun_stolen_bool, \"gun_type\": row['gun_type']}\n",
    "            gun_id = get_or_insert_id(conn, cursor, gun_id_dict, 'Gun', gun_key_dict)\n",
    "\n",
    "            participant_key = {\n",
    "                \"age_group\": row['participant_age_group'],\n",
    "                \"gender\": row['participant_gender'],\n",
    "                \"type\": row['participant_type'],\n",
    "                \"status\": row['participant_status']\n",
    "            }\n",
    "            participant_id = get_or_insert_id(conn, cursor, partecipant_id_dict, 'Partecipant', participant_key)\n",
    "\n",
    "            latitude, longitude = float(row['latitude']), float(row['longitude'])\n",
    "            location_info = get_location_info(latitude, longitude)\n",
    "            city = location_info[\"city\"]\n",
    "            state = location_info[\"state\"]\n",
    "            country = \"United States\"\n",
    "            continent = \"North America\"\n",
    "            geo_key = {\n",
    "                \"latitude\": str(latitude), \"longitude\": str(longitude), \"city\": city, \"state\": state, \"country\": country,\n",
    "                \"continent\": continent\n",
    "            }\n",
    "            geo_id = get_or_insert_id(conn, cursor, geo_id_dict, 'Geography', geo_key)\n",
    "\n",
    "            date_id = int(row['date_fk'])\n",
    "            date_value = date_mapping[date_id]\n",
    "            date, day, month, year, quarter, day_of_week = compute_date_data(date_value)\n",
    "            date_key = {\n",
    "                \"date_id\": date_id, \"the_date\": date, \"the_day\": day, \"the_month\": month, \"the_year\": year,\n",
    "                \"quarter\": quarter, \"day_of_week\": day_of_week\n",
    "            }\n",
    "            insert_data_with_ID(conn, cursor, date_id_dict, \"Date\", date_key)\n",
    "\n",
    "            incident_id = int(row['incident_id'])\n",
    "            incident_key = {\"incident_id\": incident_id}\n",
    "            insert_data_with_ID(conn, cursor, incident_id_dict, \"Incident\", incident_key)\n",
    "\n",
    "            custody_id = row['custody_id']\n",
    "            custody_key = {\n",
    "                \"custody_id\": custody_id, \"partecipant_id\": participant_id, \"gun_id\": gun_id, \"geo_id\": geo_id,\n",
    "                \"date_id\": date_id, \"crime_gravity\": compute_crime_gravity(row), \"incident_id\": incident_id\n",
    "            }\n",
    "            insert_data_with_ID(conn, cursor, custody_id_dict, 'Custody', custody_key)\n",
    "\n",
    "            # Increment the row count\n",
    "            row_count += 1\n",
    "\n",
    "            # Commit in batches of 1000\n",
    "            if row_count % batch_size == 0:\n",
    "                print(row_count)\n",
    "                conn.commit()\n",
    "\n",
    "# Commit any remaining records\n",
    "conn.commit()\n",
    "\n",
    "# Call the function with the appropriate arguments\n",
    "split_and_integrate('Police.csv')\n",
    "\n",
    "# Close the database connection when done\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert data into the database in bulk\n",
    "def bulk_insert_data(conn, cursor, table_name, columns, values_list):\n",
    "    placeholders = ', '.join(['?' for _ in range(len(columns))])\n",
    "    insert_query = f'INSERT INTO {table_name} ({columns}) VALUES ({placeholders});'\n",
    "    cursor.executemany(insert_query, values_list)\n",
    "    conn.commit()\n",
    "\n",
    "# Function to get or insert ID in bulk\n",
    "def bulk_get_or_insert_id(conn, cursor, id_dict, table_name, key_dict_list):\n",
    "    key_tuples = [tuple(key_dict.values()) for key_dict in key_dict_list]\n",
    "    existing_keys = set(key_tuples) & set(id_dict.keys())\n",
    "    new_keys = set(key_tuples) - existing_keys\n",
    "\n",
    "    # Insert new keys\n",
    "    new_key_dicts = [key_dict_list[key_tuples.index(key)] for key in new_keys]\n",
    "    columns = ', '.join(new_key_dicts[0].keys())\n",
    "    bulk_insert_data(conn, cursor, table_name, columns, [tuple(key_dict.values()) for key_dict in new_key_dicts])\n",
    "\n",
    "    # Fetch IDs of all keys\n",
    "    cursor.execute(f'SELECT * FROM {table_name} WHERE ({columns}) IN ({\", \".join([\"?\" for _ in range(len(new_key_dicts[0]))])});', [tuple(key_dict.values()) for key_dict in key_dict_list])\n",
    "    new_ids = cursor.fetchall()\n",
    "\n",
    "    # Update id_dict with new IDs\n",
    "    for i, key_tuple in enumerate(new_keys):\n",
    "        id_dict[key_tuple] = new_ids[i][0]\n",
    "\n",
    "    # Return IDs for all keys\n",
    "    return [id_dict[key_tuple] for key_tuple in key_tuples]\n",
    "\n",
    "# Function to split and integrate data\n",
    "def split_and_integrate(csv_file):\n",
    "    # Parse dates.xml to get date mapping\n",
    "    date_mapping = parse_dates_xml('dates.xml')\n",
    "\n",
    "    # List of table names in your database\n",
    "    table_names = ['Custody', 'Geography', 'Gun', 'Date', 'Incident', 'Partecipant']\n",
    "\n",
    "    # Clean the tables by deleting all records\n",
    "    for table_name in table_names:\n",
    "        cursor.execute(f'DELETE FROM {table_name}')\n",
    "        conn.commit()\n",
    "\n",
    "    # Read and process Police.csv\n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        next(reader)  # Skip the header row\n",
    "\n",
    "        # Set the batch size\n",
    "        batch_size = 50\n",
    "        batch_data = []\n",
    "\n",
    "        for row in reader:\n",
    "            custody_id, participant_age_group, participant_gender, participant_status, participant_type, latitude, longitude, gun_stolen, gun_type, incident_id, date_fk = row\n",
    "\n",
    "            gun_stolen_bool = 1 if row[\"gun_stolen\"] == 'Stolen' else 0\n",
    "            gun_key_dict = {\"is_stolen\": gun_stolen_bool, \"gun_type\": row['gun_type']}\n",
    "            gun_id = get_or_insert_id(conn, cursor, gun_id_dict, 'Gun', [gun_key_dict])[0]\n",
    "\n",
    "            partecipant_key = {\n",
    "                \"age_group\": row['participant_age_group'],\n",
    "                \"gender\": row['participant_gender'],\n",
    "                \"type\": row['participant_type'],\n",
    "                \"status\": row['participant_status']\n",
    "            }\n",
    "            partecipant_id = get_or_insert_id(conn, cursor, partecipant_id_dict, 'Partecipant', [partecipant_key])[0]\n",
    "\n",
    "            # Get location information from latitude and longitude\n",
    "            latitude, longitude = float(row['latitude']), float(row['longitude'])\n",
    "            location_info = get_location_info(latitude, longitude)\n",
    "            city = location_info[\"city\"]\n",
    "            state = location_info[\"state\"]\n",
    "            country = \"United States\"\n",
    "            continent = \"North America\"\n",
    "            geo_key = {\n",
    "                \"latitude\": str(latitude), \"longitude\": str(longitude), \"city\": city, \"state\": state, \"country\": country,\n",
    "                \"continent\": continent\n",
    "            }\n",
    "            geo_id = get_or_insert_id(conn, cursor, geo_id_dict, 'Geography', [geo_key])[0]\n",
    "\n",
    "            # Normal ID, No Incremental Tables:\n",
    "            date_id = int(row['date_fk'])\n",
    "            date_value = date_mapping[date_id]\n",
    "            date, day, month, year, quarter, day_of_week = compute_date_data(date_value)\n",
    "            date_key = {\n",
    "                \"date_id\": date_id, \"the_date\": date, \"the_day\": day, \"the_month\": month, \"the_year\": year,\n",
    "                \"quarter\": quarter, \"day_of_week\": day_of_week\n",
    "            }\n",
    "            insert_data_with_ID(conn, cursor, date_id_dict, \"Date\", date_key)\n",
    "\n",
    "            incident_id = int(row['incident_id'])\n",
    "            incident_key = {\"incident_id\": incident_id}\n",
    "            insert_data_with_ID(conn, cursor, incident_id_dict, \"Incident\", incident_key)\n",
    "\n",
    "            custody_id = row['custody_id']\n",
    "            custody_key = {\n",
    "                \"custody_id\": custody_id, \"partecipant_id\": partecipant_id, \"gun_id\": gun_id, \"geo_id\": geo_id,\n",
    "                \"date_id\": date_id, \"crime_gravity\": compute_crime_gravity(row), \"incident_id \": incident_id\n",
    "            }\n",
    "\n",
    "            # Append data to the batch\n",
    "            batch_data.append(tuple(custody_key.values()))\n",
    "\n",
    "            # Increment the row count\n",
    "            row_count += 1\n",
    "\n",
    "            # Commit in batches\n",
    "            if row_count % batch_size == 0:\n",
    "                bulk_insert_data(conn, cursor, 'Custody', columns, batch_data)\n",
    "                batch_data = []  # Reset batch data\n",
    "\n",
    "    # Commit any remaining records\n",
    "    bulk_insert_data(conn, cursor, 'Custody', columns, batch_data)\n",
    "\n",
    "# ... (existing code)\n",
    "\n",
    "# Call the function with the appropriate arguments\n",
    "split_and_integrate('Police.csv')\n",
    "\n",
    "# Close the database connection when done\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (your existing code)\n",
    "\n",
    "# Initialize dictionaries to store existing IDs\n",
    "geo_id_dict = {}\n",
    "gun_id_dict = {}\n",
    "partecipant_id_dict = {}\n",
    "date_id_dict = {}\n",
    "incident_id_dict = {}\n",
    "custody_id_dict = {}\n",
    "\n",
    "# Retrieve existing IDs from the 'Geography' table\n",
    "cursor.execute(\"SELECT geo_id, latitude, longitude FROM Geography\")\n",
    "for row in cursor.fetchall():\n",
    "    geo_id = row[0]\n",
    "    latitude = row[1]\n",
    "    longitude = row[2]\n",
    "    geo_key = {\"latitude\": str(latitude), \"longitude\": str(longitude)}\n",
    "    geo_id_dict[geo_id] = geo_key\n",
    "\n",
    "# Retrieve existing IDs from the 'Gun' table\n",
    "cursor.execute(\"SELECT gun_id, is_stolen, gun_type FROM Gun\")\n",
    "for row in cursor.fetchall():\n",
    "    gun_id = row[0]\n",
    "    is_stolen = row[1]\n",
    "    gun_type = row[2]\n",
    "    gun_key = {\"is_stolen\": is_stolen, \"gun_type\": gun_type}\n",
    "    gun_id_dict[gun_id] = gun_key\n",
    "\n",
    "# Retrieve existing IDs from the 'Partecipant' table\n",
    "cursor.execute(\"SELECT partecipant_id, age_group, gender, type, status FROM Partecipant\")\n",
    "for row in cursor.fetchall():\n",
    "    partecipant_id = row[0]\n",
    "    age_group = row[1]\n",
    "    gender = row[2]\n",
    "    participant_type = row[3]\n",
    "    status = row[4]\n",
    "    partecipant_key = {\"age_group\": age_group, \"gender\": gender, \"type\": participant_type, \"status\": status}\n",
    "    partecipant_id_dict[partecipant_id] = partecipant_key\n",
    "\n",
    "# Retrieve existing IDs from the 'Date' table\n",
    "cursor.execute(\"SELECT date_id, the_date, the_day, the_month, the_year, quarter, day_of_week FROM Date\")\n",
    "for row in cursor.fetchall():\n",
    "    date_id = row[0]\n",
    "    the_date = row[1]\n",
    "    the_day = row[2]\n",
    "    the_month = row[3]\n",
    "    the_year = row[4]\n",
    "    quarter = row[5]\n",
    "    day_of_week = row[6]\n",
    "    date_key = {\n",
    "        \"date_id\": date_id, \"the_date\": the_date, \"the_day\": the_day, \"the_month\": the_month,\n",
    "        \"the_year\": the_year, \"quarter\": quarter, \"day_of_week\": day_of_week\n",
    "    }\n",
    "    date_id_dict[date_id] = date_key\n",
    "\n",
    "# Retrieve existing IDs from the 'Incident' table\n",
    "cursor.execute(\"SELECT incident_id FROM Incident\")\n",
    "for row in cursor.fetchall():\n",
    "    incident_id = row[0]\n",
    "    incident_key = {\"incident_id\": incident_id}\n",
    "    incident_id_dict[incident_id] = incident_key\n",
    "\n",
    "# Retrieve existing IDs from the 'Custody' table\n",
    "cursor.execute(\"SELECT custody_id, partecipant_id, gun_id, geo_id, date_id, crime_gravity, incident_id FROM Custody\")\n",
    "for row in cursor.fetchall():\n",
    "    custody_id = row[0]\n",
    "    partecipant_id = row[1]\n",
    "    gun_id = row[2]\n",
    "    geo_id = row[3]\n",
    "    date_id = row[4]\n",
    "    crime_gravity = row[5]\n",
    "    incident_id = row[6]\n",
    "    custody_key = {\n",
    "        \"custody_id\": custody_id, \"partecipant_id\": partecipant_id, \"gun_id\": gun_id, \"geo_id\": geo_id,\n",
    "        \"date_id\": date_id, \"crime_gravity\": crime_gravity, \"incident_id\": incident_id\n",
    "    }\n",
    "    custody_id_dict[custody_id] = custody_key\n",
    "\n",
    "# ... (continue with your existing code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import sqlite3  # Assuming you are using SQLite, you can replace this with your database library\n",
    "from decimal import Decimal as Decimal\n",
    "\n",
    "\n",
    "# Function to get existing IDs from the database and populate the dictionary\n",
    "def get_existing_ids(conn, cursor, table_name, key_columns):\n",
    "    existing_ids = {}\n",
    "    cursor.execute(f'SELECT * FROM {table_name}')\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Print the rows for debugging\n",
    "    print(\"Rows:\", rows)\n",
    "\n",
    "    for row in rows:\n",
    "        # Convert Decimal values to float\n",
    "        key = tuple(float(row[key_column]) if isinstance(row[key_column], Decimal) else row[key_column] for key_column in key_columns)\n",
    "        existing_ids[key] = row[-1]  # Assuming the ID is the last element in the tuple\n",
    "    return existing_ids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to insert data with ID into the database and update the dictionary\n",
    "def insert_data_with_id(conn, cursor, id_dict, table_name, data):\n",
    "    key = tuple(data[key_column] for key_column in id_dict[table_name]['key_columns'])\n",
    "    if key in id_dict[table_name]['existing_ids']:\n",
    "        data['id'] = id_dict[table_name]['existing_ids'][key]\n",
    "    else:\n",
    "        columns = ', '.join(data.keys())\n",
    "        placeholders = ', '.join(['?' for _ in range(len(data))])\n",
    "        cursor.execute(f'INSERT INTO {table_name} ({columns}) VALUES ({placeholders})', list(data.values()))\n",
    "        conn.commit()\n",
    "        new_id = cursor.lastrowid\n",
    "        data['id'] = new_id\n",
    "        id_dict[table_name]['existing_ids'][key] = new_id\n",
    "    return data['id']\n",
    "\n",
    "# Your existing code...\n",
    "\n",
    "# Initialize dictionaries to store existing IDs\n",
    "geo_id_dict = {'existing_ids': get_existing_ids(conn, cursor, 'Geography', [1, 2])}  # Use column indices\n",
    "gun_id_dict = {'existing_ids': get_existing_ids(conn, cursor, 'Gun', [1, 2])}  # Use column indices\n",
    "partecipant_id_dict = {'existing_ids': get_existing_ids(conn, cursor, 'Partecipant', [1, 2, 3, 4])}  # Use column indices\n",
    "date_id_dict = {'existing_ids': get_existing_ids(conn, cursor, 'Date', [0, 1, 2, 3, 4, 5])}  # Use column indices\n",
    "incident_id_dict = {'existing_ids': get_existing_ids(conn, cursor, 'Incident', [0])}  # Use column indices\n",
    "custody_id_dict = {'existing_ids': get_existing_ids(conn, cursor, 'Custody', [0, 1, 2, 3, 4, 5])}  # Use column indices\n",
    "\n",
    "\n",
    "# Your existing code...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
